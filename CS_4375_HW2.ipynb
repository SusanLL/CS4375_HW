{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **FFNN Model**"
      ],
      "metadata": {
        "id": "zynZP-mhb0mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from argparse import ArgumentParser"
      ],
      "metadata": {
        "id": "De7E1NU7uEF9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk = '<UNK>'\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, input_dim, h):\n",
        "        super(FFNN, self).__init__()\n",
        "        self.h = h\n",
        "        self.W1 = nn.Linear(input_dim, h)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.output_dim = 5\n",
        "        self.W2 = nn.Linear(h, self.output_dim)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.loss = nn.NLLLoss()\n",
        "\n",
        "    def compute_Loss(self, predicted_vector, gold_label):\n",
        "        return self.loss(predicted_vector, gold_label)\n",
        "\n",
        "    def forward(self, input_vector):\n",
        "       # Ensure input_vector has a batch dimension\n",
        "        if input_vector.dim() == 1:\n",
        "          input_vector = input_vector.unsqueeze(0)\n",
        "        # obtain first hidden layer representation\n",
        "        hidden_rep = self.activation(self.W1(input_vector))\n",
        "\n",
        "        # obtain output layer representation\n",
        "        z = self.W2(hidden_rep)\n",
        "\n",
        "        # obtain probability dist.\n",
        "        predicted_vector = self.softmax(z)\n",
        "\n",
        "        return predicted_vector"
      ],
      "metadata": {
        "id": "2ZAYPhgRa017"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns:\n",
        "# vocab = A set of strings corresponding to the vocabulary\n",
        "def make_vocab(data):\n",
        "    vocab = set()\n",
        "    for document, _ in data:\n",
        "        for word in document:\n",
        "            vocab.add(word)\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "rP3pkO6Uz8Qy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns:\n",
        "# vocab = A set of strings corresponding to the vocabulary including <UNK>\n",
        "# word2index = A dictionary mapping word/token to its index (a number in 0, ..., V - 1)\n",
        "# index2word = A dictionary inverting the mapping of word2index\n",
        "def make_indices(vocab):\n",
        "    vocab_list = sorted(vocab)\n",
        "    vocab_list.append(unk)\n",
        "    word2index = {}\n",
        "    index2word = {}\n",
        "    for index, word in enumerate(vocab_list):\n",
        "        word2index[word] = index\n",
        "        index2word[index] = word\n",
        "    vocab.add(unk)\n",
        "    return vocab, word2index, index2word"
      ],
      "metadata": {
        "id": "c_An4sPe0BSX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns:\n",
        "# vectorized_data = A list of pairs (vector representation of input, y)\n",
        "def convert_to_vector_representation(data, word2index):\n",
        "    vectorized_data = []\n",
        "    for document, y in data:\n",
        "        vector = torch.zeros(len(word2index))\n",
        "        for word in document:\n",
        "            index = word2index.get(word, word2index[unk])\n",
        "            vector[index] += 1\n",
        "        vectorized_data.append((vector, y))\n",
        "    return vectorized_data"
      ],
      "metadata": {
        "id": "1zi21Vww0FUv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(train_data, val_data):\n",
        "    with open(train_data) as training_f:\n",
        "        training = json.load(training_f)\n",
        "    with open(val_data) as valid_f:\n",
        "        validation = json.load(valid_f)\n",
        "\n",
        "    tra = []\n",
        "    val = []\n",
        "    for elt in training:\n",
        "        tra.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
        "    for elt in validation:\n",
        "        val.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
        "\n",
        "    return tra, val"
      ],
      "metadata": {
        "id": "PCxmqOMj0Kg_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz5mdsUusE3c",
        "outputId": "ed252281-d6f7-47af-b5aa-68a30bc38832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== Loading data ==========\n",
            "========== Vectorizing data ==========\n",
            "========== Training for 10 epochs ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:17<00:00, 29.22it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 111.10it/s]\n",
            "100%|██████████| 500/500 [00:17<00:00, 28.28it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 132.42it/s]\n",
            "100%|██████████| 500/500 [00:18<00:00, 27.38it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 132.53it/s]\n",
            "100%|██████████| 500/500 [00:17<00:00, 28.99it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 127.68it/s]\n",
            "100%|██████████| 500/500 [00:17<00:00, 28.57it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 102.25it/s]\n",
            "100%|██████████| 500/500 [00:18<00:00, 27.40it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 130.62it/s]\n",
            "100%|██████████| 500/500 [00:17<00:00, 28.81it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 132.57it/s]\n",
            "100%|██████████| 500/500 [00:16<00:00, 29.45it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 131.39it/s]\n",
            "100%|██████████| 500/500 [00:17<00:00, 28.55it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 108.26it/s]\n",
            "100%|██████████| 500/500 [00:17<00:00, 28.63it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 132.23it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument(\"-hd\", \"--hidden_dim\", type=int, required=True, help=\"hidden_dim\")\n",
        "    parser.add_argument(\"-e\", \"--epochs\", type=int, required=True, help=\"num of epochs to train\")\n",
        "    parser.add_argument(\"--train_data\", required=True, help=\"/content/FFNN/training.json\")\n",
        "    parser.add_argument(\"--val_data\", required=True, help=\"/content/FFNN/validation.json\")\n",
        "    parser.add_argument(\"--test_data\", default=\"to fill\", help=\"/content/FFNN/test.json\")\n",
        "    parser.add_argument('--do_train', action='store_true')\n",
        "\n",
        "    arg_list = [\n",
        "        \"--hidden_dim\", \"16\",\n",
        "        \"--epochs\", \"10\",\n",
        "        \"--train_data\", \"/content/FFNN/training.json\",\n",
        "        \"--val_data\", \"/content/FFNN/validation.json\"\n",
        "    ]\n",
        "    args = parser.parse_args(arg_list)\n",
        "\n",
        "    # Fix random seeds\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Load data\n",
        "    print(\"========== Loading data ==========\")\n",
        "    train_data, valid_data = load_data(args.train_data, args.val_data)\n",
        "    vocab = make_vocab(train_data)\n",
        "    vocab, word2index, index2word = make_indices(vocab)\n",
        "\n",
        "    print(\"========== Vectorizing data ==========\")\n",
        "    train_data = convert_to_vector_representation(train_data, word2index)\n",
        "    valid_data = convert_to_vector_representation(valid_data, word2index)\n",
        "\n",
        "    model = FFNN(input_dim=len(vocab), h=args.hidden_dim)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n",
        "\n",
        "    # Ensure 'results' directory exists\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    # Open the output file in write mode\n",
        "    with open(\"results/FFNNtest.out\", \"w\") as f:\n",
        "        f.write(\"Training and Validation Results\\n\")\n",
        "        f.write(\"==============================\\n\")\n",
        "\n",
        "        print(\"========== Training for {} epochs ==========\".format(args.epochs))\n",
        "        for epoch in range(args.epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            start_time = time.time()\n",
        "            random.shuffle(train_data)\n",
        "            minibatch_size = 16\n",
        "            N = len(train_data)\n",
        "\n",
        "            for minibatch_index in tqdm(range(N // minibatch_size)):\n",
        "                optimizer.zero_grad()\n",
        "                loss = None\n",
        "                for example_index in range(minibatch_size):\n",
        "                    input_vector, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n",
        "                    predicted_vector = model(input_vector)\n",
        "                    predicted_label = torch.argmax(predicted_vector)\n",
        "                    correct += int(predicted_label == gold_label)\n",
        "                    total += 1\n",
        "                    example_loss = model.compute_Loss(predicted_vector.view(1, -1), torch.tensor([gold_label]))\n",
        "                    if loss is None:\n",
        "                        loss = example_loss\n",
        "                    else:\n",
        "                        loss += example_loss\n",
        "                loss = loss / minibatch_size\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            training_accuracy = correct / total\n",
        "            epoch_time = time.time() - start_time\n",
        "            f.write(f\"Epoch {epoch + 1} - Training accuracy: {training_accuracy:.4f}, Training time: {epoch_time:.2f} seconds\\n\")\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            with torch.no_grad():\n",
        "                for minibatch_index in tqdm(range(len(valid_data) // minibatch_size)):\n",
        "                    loss = None\n",
        "                    for example_index in range(minibatch_size):\n",
        "                        input_vector, gold_label = valid_data[minibatch_index * minibatch_size + example_index]\n",
        "                        predicted_vector = model(input_vector)\n",
        "                        predicted_label = torch.argmax(predicted_vector)\n",
        "                        correct += int(predicted_label == gold_label)\n",
        "                        total += 1\n",
        "                        example_loss = model.compute_Loss(predicted_vector.view(1, -1), torch.tensor([gold_label]))\n",
        "                        if loss is None:\n",
        "                            loss = example_loss\n",
        "                        else:\n",
        "                            loss += example_loss\n",
        "                    loss = loss / minibatch_size\n",
        "\n",
        "            validation_accuracy = correct / total\n",
        "            f.write(f\"Epoch {epoch + 1} - Validation accuracy: {validation_accuracy:.4f}\\n\")\n",
        "\n",
        "        f.write(\"Training and validation complete.\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load JSON data from the specified file path.\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def calculate_statistics(data):\n",
        "    \"\"\"Calculate number of examples, average words per review, and star rating distribution.\"\"\"\n",
        "    num_examples = len(data)\n",
        "    word_counts = [len(review['text'].split()) for review in data]\n",
        "    avg_words_per_review = np.mean(word_counts)\n",
        "\n",
        "    # Count star ratings distribution\n",
        "    star_ratings = [int(review['stars']) for review in data]\n",
        "    unique, counts = np.unique(star_ratings, return_counts=True)\n",
        "    star_distribution = dict(zip(unique, counts))\n",
        "\n",
        "    # Convert star distribution to percentages\n",
        "    star_distribution_percentage = {star: count / num_examples * 100 for star, count in star_distribution.items()}\n",
        "\n",
        "    return num_examples, avg_words_per_review, star_distribution_percentage\n",
        "\n",
        "# Load each dataset and calculate statistics\n",
        "datasets = {\n",
        "    \"Training\": \"training.json\",\n",
        "    \"Validation\": \"validation.json\",\n",
        "    \"Test\": \"test.json\"\n",
        "}\n",
        "\n",
        "for dataset_name, file_path in datasets.items():\n",
        "    data = load_data(file_path)\n",
        "    num_examples, avg_words, star_distribution = calculate_statistics(data)\n",
        "\n",
        "    # Print out the statistics\n",
        "    print(f\"{dataset_name} Set:\")\n",
        "    print(f\"  Number of Examples: {num_examples}\")\n",
        "    print(f\"  Average Words per Review: {avg_words:.2f}\")\n",
        "    print(f\"  Star Ratings Distribution: {star_distribution}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "iO7Zdsf4dBCf",
        "outputId": "03eca96a-ca3a-4d8d-b896-92458e6f7111"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'training.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-fb9ce4f59ee0>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mnum_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstar_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-fb9ce4f59ee0>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\"Load JSON data from the specified file path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3bU-20P_clzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN Model**"
      ],
      "metadata": {
        "id": "s0F9_lXjb7hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import string\n",
        "from argparse import ArgumentParser\n",
        "import pickle"
      ],
      "metadata": {
        "id": "1ARnhpgBkGVA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk = '<UNK>'\n",
        "# Consult the PyTorch documentation for information on the functions used below:\n",
        "# https://pytorch.org/docs/stable/torch.html\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, h):  # Add relevant parameters\n",
        "        super(RNN, self).__init__()\n",
        "        self.h = h\n",
        "        self.numOfLayer = 1\n",
        "        self.rnn = nn.RNN(input_dim, h, self.numOfLayer, nonlinearity='tanh')\n",
        "        self.W = nn.Linear(h, 5)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.loss = nn.NLLLoss()\n",
        "\n",
        "    def compute_Loss(self, predicted_vector, gold_label):\n",
        "        return self.loss(predicted_vector, gold_label)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "      # Step 1: Pass inputs through the RNN layer to obtain hidden layer representation\n",
        "      _, hidden = self.rnn(inputs)  # Obtain the final hidden state from the RNN\n",
        "\n",
        "      # Step 2: Pass the hidden state through a linear layer to obtain output layer representations\n",
        "      z = self.W(hidden[-1])  # Use the final hidden state for prediction\n",
        "\n",
        "      # Step 3: Obtain probability distribution over classes\n",
        "      predicted_vector = self.softmax(z)\n",
        "\n",
        "      return predicted_vector"
      ],
      "metadata": {
        "id": "jH8J1ybGkMB2"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(train_data_path, val_data_path):\n",
        "    \"\"\"Load JSON data from the specified training and validation file paths.\"\"\"\n",
        "    with open(train_data_path) as training_f:\n",
        "        training = json.load(training_f)\n",
        "    with open(val_data_path) as valid_f:\n",
        "        validation = json.load(valid_f)\n",
        "\n",
        "    tra = [(elt[\"text\"].split(), int(elt[\"stars\"] - 1)) for elt in training]\n",
        "    val = [(elt[\"text\"].split(), int(elt[\"stars\"] - 1)) for elt in validation]\n",
        "    return tra, val"
      ],
      "metadata": {
        "id": "dCHBVJzEkOOg"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "try:\n",
        "    with open('word_embedding.pkl', 'rb') as f:\n",
        "        word_embedding = pickle.load(f)\n",
        "    print(\"File loaded successfully.\")\n",
        "except pickle.UnpicklingError:\n",
        "    print(\"UnpicklingError: The file is not a valid pickle file.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBFIkTvjp8I6",
        "outputId": "189de7cd-6a06-4405-bd3c-d38e194bfa47"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Create a random word embedding dictionary\n",
        "vocab = [\"<UNK>\", \"example\", \"words\", \"here\"]\n",
        "word_embedding = {word: np.random.rand(50) for word in vocab}\n",
        "\n",
        "# Save to pickle file\n",
        "with open('word_embedding.pkl', 'wb') as f:\n",
        "    pickle.dump(word_embedding, f)"
      ],
      "metadata": {
        "id": "RWpicRo2ZmJX"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if running in an interactive environment\n",
        "    if 'ipykernel_launcher' in sys.argv[0] or 'google.colab' in sys.modules:\n",
        "        # Set default values for arguments in interactive environments\n",
        "        class Args:\n",
        "            hidden_dim = 32\n",
        "            epochs = 10\n",
        "            train_data = \"/content/RNN/training.json\"\n",
        "            val_data = \"/content/RNN/validation.json\"\n",
        "            test_data = \"/content/RNN/test.json\"\n",
        "            do_train = True\n",
        "        args = Args()\n",
        "    else:\n",
        "        parser = ArgumentParser()\n",
        "        parser.add_argument(\"-hd\", \"--hidden_dim\", type=int, required=True, help=\"hidden_dim\")\n",
        "        parser.add_argument(\"-e\", \"--epochs\", type=int, required=True, help=\"num of epochs to train\")\n",
        "        parser.add_argument(\"--train_data\", required=True, help=\"path to training data\")\n",
        "        parser.add_argument(\"--val_data\", required=True, help=\"path to validation data\")\n",
        "        parser.add_argument(\"--test_data\", default=\"to fill\", help=\"path to test data\")\n",
        "        parser.add_argument('--do_train', action='store_true')\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    print(\"========== Loading data ==========\")\n",
        "    train_data, valid_data = load_data(args.train_data, args.val_data)\n",
        "\n",
        "    print(\"========== Vectorizing data ==========\")\n",
        "    model = RNN(50, args.hidden_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Load pre-trained embeddings\n",
        "    with open('./word_embedding.pkl', 'rb') as f:\n",
        "        word_embedding = pickle.load(f)\n",
        "\n",
        "    stopping_condition = False\n",
        "    epoch = 0\n",
        "    last_train_accuracy = 0\n",
        "    last_validation_accuracy = 0\n",
        "\n",
        "    while not stopping_condition:\n",
        "      random.shuffle(train_data)\n",
        "      model.train()\n",
        "      # You will need further code to operationalize training, ffnn.py may be helpful\n",
        "      print(\"Training started for epoch {}\".format(epoch + 1))\n",
        "      train_data = train_data\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      minibatch_size = 16\n",
        "      N = len(train_data)\n",
        "\n",
        "      loss_total = 0\n",
        "      loss_count = 0\n",
        "      for minibatch_index in tqdm(range(N // minibatch_size)):\n",
        "          optimizer.zero_grad()\n",
        "          loss = None\n",
        "          for example_index in range(minibatch_size):\n",
        "              input_words, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n",
        "              input_words = \" \".join(input_words)\n",
        "\n",
        "              # Remove punctuation\n",
        "              input_words = input_words.translate(input_words.maketrans(\"\", \"\", string.punctuation)).split()\n",
        "\n",
        "              # Look up word embedding dictionary and handle missing embeddings with 'unk'\n",
        "              vectors = [torch.tensor(word_embedding[i.lower()], dtype=torch.float32) if i.lower() in word_embedding else torch.tensor(word_embedding[unk], dtype=torch.float32) for i in input_words]\n",
        "\n",
        "              # Stack the tensors into a single tensor of shape [sequence_length, embedding_dim]\n",
        "              vectors = torch.stack(vectors).view(len(vectors), 1, -1).float()\n",
        "              output = model(vectors)\n",
        "\n",
        "              # Get loss\n",
        "              example_loss = model.compute_Loss(output.view(1,-1), torch.tensor([gold_label]))\n",
        "\n",
        "              # Get predicted label\n",
        "              predicted_label = torch.argmax(output)\n",
        "\n",
        "              correct += int(predicted_label == gold_label)\n",
        "              # print(predicted_label, gold_label)\n",
        "              total += 1\n",
        "              if loss is None:\n",
        "                  loss = example_loss\n",
        "              else:\n",
        "                  loss += example_loss\n",
        "\n",
        "          loss = loss / minibatch_size\n",
        "          loss_total += loss.data\n",
        "          loss_count += 1\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "      print(loss_total/loss_count)\n",
        "      print(\"Training completed for epoch {}\".format(epoch + 1))\n",
        "      print(\"Training accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
        "      trainning_accuracy = correct/total\n",
        "\n",
        "\n",
        "      model.eval()\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      random.shuffle(valid_data)\n",
        "      print(\"Validation started for epoch {}\".format(epoch + 1))\n",
        "      valid_data = valid_data\n",
        "\n",
        "      for input_words, gold_label in tqdm(valid_data):\n",
        "          input_words = \" \".join(input_words)\n",
        "          input_words = input_words.translate(input_words.maketrans(\"\", \"\", string.punctuation)).split()\n",
        "          # Look up word embeddings and ensure each tensor is float32\n",
        "          vectors = [torch.tensor(word_embedding[i.lower()], dtype=torch.float32) if i.lower() in word_embedding else torch.tensor(word_embedding[unk], dtype=torch.float32) for i in input_words]\n",
        "\n",
        "          # Stack the tensors into a single tensor with dtype=torch.float32\n",
        "          vectors = torch.stack(vectors).view(len(vectors), 1, -1)\n",
        "          output = model(vectors)\n",
        "          predicted_label = torch.argmax(output)\n",
        "          correct += int(predicted_label == gold_label)\n",
        "          total += 1\n",
        "          # print(predicted_label, gold_label)\n",
        "      print(\"Validation completed for epoch {}\".format(epoch + 1))\n",
        "      print(\"Validation accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
        "      validation_accuracy = correct/total\n",
        "\n",
        "      if validation_accuracy < last_validation_accuracy and trainning_accuracy > last_train_accuracy:\n",
        "          stopping_condition=True\n",
        "          print(\"Training done to avoid overfitting!\")\n",
        "          print(\"Best validation accuracy is:\", last_validation_accuracy)\n",
        "      else:\n",
        "          last_validation_accuracy = validation_accuracy\n",
        "          last_train_accuracy = trainning_accuracy\n",
        "\n",
        "      epoch += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJw-bbUFb-jf",
        "outputId": "9cdc8de2-2258-4dae-fceb-23d0f925430e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== Loading data ==========\n",
            "========== Vectorizing data ==========\n",
            "Training started for epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:03<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.6440)\n",
            "Training completed for epoch 1\n",
            "Training accuracy for epoch 1: 0.206875\n",
            "Validation started for epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:05<00:00, 135.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 1\n",
            "Validation accuracy for epoch 1: 0.4\n",
            "Training started for epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [03:56<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.6399)\n",
            "Training completed for epoch 2\n",
            "Training accuracy for epoch 2: 0.2045625\n",
            "Validation started for epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:04<00:00, 160.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 2\n",
            "Validation accuracy for epoch 2: 0.4\n",
            "Training started for epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:01<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.6472)\n",
            "Training completed for epoch 3\n",
            "Training accuracy for epoch 3: 0.19775\n",
            "Validation started for epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:06<00:00, 125.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 3\n",
            "Validation accuracy for epoch 3: 0.01\n",
            "Training started for epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:00<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.6444)\n",
            "Training completed for epoch 4\n",
            "Training accuracy for epoch 4: 0.2\n",
            "Validation started for epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:05<00:00, 152.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 4\n",
            "Validation accuracy for epoch 4: 0.4\n",
            "Training started for epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [03:57<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.6434)\n",
            "Training completed for epoch 5\n",
            "Training accuracy for epoch 5: 0.2028125\n",
            "Validation started for epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:05<00:00, 154.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation completed for epoch 5\n",
            "Validation accuracy for epoch 5: 0.01\n",
            "Training done to avoid overfitting!\n",
            "Best validation accuracy is: 0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}